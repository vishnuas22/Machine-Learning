{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import lightgbm as lgb\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.datasets import make_classification\n",
    "\n",
    "from scipy.sparse import csr_matrix\n",
    "\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate sample dataset\n",
    "\n",
    "X, y = make_classification(n_samples=1000, n_features=10, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1Ô∏è‚É£ Using LightGBM‚Äôs Dataset Storage Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert dataset to LightGBM binary format\n",
    "\n",
    "train_data = lgb.Dataset(X_train, label=y_train)\n",
    "\n",
    "train_data.save_binary('train_data.bin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load from binary format\n",
    "\n",
    "train_data_bin = lgb.Dataset('train_data.bin')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚úÖ Key Benefit: Saves memory & loads faster than traditional file formats."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train using optimized dataset\n",
    "\n",
    "lgb_clf = lgb.train({\n",
    "\n",
    "    'objective' : 'binary'\n",
    "}\n",
    "train_data_bin,\n",
    "num_boost_round=10\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2Ô∏è‚É£ Using Out-of-Core Training for Memory Efficiency\n",
    "\n",
    "When data is too large to fit into RAM, we can train in batches from disk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = lgb.Dataset('large_dataset.csv', \n",
    "params={'max_bin':255},\n",
    "free_row_data = False\n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3Ô∏è‚É£  Reducing Memory Usage with Sparse Matrices\n",
    "\n",
    "Using scipy.sparse matrices saves memory by storing only non-zero values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert dataset to sparse format\n",
    "\n",
    "X_train_sparse = csr_matrix(X_train)\n",
    "X_test_sparse = csr_matrix(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train LightGBM with sparse data\n",
    "\n",
    "lgb_sparse = lgb.LGBMClassifier(n_estimators=100)\n",
    "\n",
    "lgb_sparse.fit(X_train_sparse, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚úÖ Key Benefit: Uses much less memory when working with sparse data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Why Use Distributed Training?\n",
    "\n",
    "When datasets become too large for a single machine, distributed training helps by spreading computations across multiple machines or GPUs.\n",
    "\n",
    "‚úÖ Key Benefits of Distributed Training in LightGBM:\n",
    "Scales across multiple nodes (parallel training on clusters)\n",
    "Efficient for huge datasets (millions or billions of rows)\n",
    "Utilizes multiple CPUs or GPUs simultaneously\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enable multi-threading\n",
    "\n",
    "params = {\n",
    "\n",
    "    'objective':'binary',\n",
    "    'num_threads' : 8, # Use 8 CPU Threads\n",
    "    'n_estimators': 200\n",
    "\n",
    "\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train with the Data\n",
    "\n",
    "lgb_clf = lgb.LGBMClassifier(**params)\n",
    "\n",
    "lgb_clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate model\n",
    "\n",
    "\n",
    "y_pred = lgb_clf.predict(X_test)\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4Ô∏è‚É£ Distributed Training Across Multiple Machines\n",
    "\n",
    "LightGBM supports distributed training using multiple machines via MPI (Message Passing Interface).\n",
    "\n",
    "\n",
    "üìå Steps to Set Up Distributed Training Across Machines\n",
    "Install LightGBM with MPI support"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install lightgbm\n",
    "sudo apt-get install libopenmpi-dev openmpi-bin\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prepare data in LightGBM format (Each machine processes a part of the data)\n",
    "Run training across multiple machines using MPI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mpirun -n 4 --host machine1,machine2,machine3,machine4 python train_lightgbm.py\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5Ô∏è‚É£ Distributed Training on GPUs\n",
    "\n",
    "LightGBM also supports GPU-accelerated distributed training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    'objective': 'binary',\n",
    "    'device': 'gpu',  # Use GPU\n",
    "    'gpu_platform_id': 0,  # Specify GPU device\n",
    "    'gpu_device_id': 0,\n",
    "    'num_leaves': 31,\n",
    "    'learning_rate': 0.05,\n",
    "    'n_estimators': 200\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lgb_clf_gpu = lgb.LGBMClassifier(**params)\n",
    "lgb_clf_gpu.fit(X_train, y_train)\n",
    "\n",
    "print(\"GPU Training Completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚úÖ Key Benefit: Train massive datasets faster using multiple GPUs."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "masterxml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
