{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb \n",
    "\n",
    "from sklearn.datasets import make_classification\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.metrics import accuracy_score, classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate synthetic data\n",
    "\n",
    "X,y = make_classification(\n",
    "    n_samples= 1000,\n",
    "    n_features=20,\n",
    "    n_informative=15,\n",
    "    n_redundant=5,\n",
    "    random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define parameters for the XGBoost model\n",
    "params = {\n",
    "    'objective': 'binary:logistic',      # Binary classification\n",
    "    'eval_metric': 'logloss',            # Evaluation metric\n",
    "    'num_boost_round' : 200,             # number of boostings (trees)\n",
    "    'learning_rate' : 0.1,               # Step size shrinkage\n",
    "    'max_depth' : 3,                     # Depth of each tree\n",
    "    'subsample' : 0.8,                   # Fraction of samples used by tree\n",
    "    'colsample_bytree' : 0.8,            # Fraction of features used by tree\n",
    "    'random_state' : 42\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to DMatrix\n",
    "\n",
    "d_train = xgb.DMatrix(X_train, label=y_train)\n",
    "\n",
    "d_test = xgb.DMatrix(X_test, label=y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up early stopping and validation\n",
    "\n",
    "evals = [(d_test, 'eval'), (d_train, 'train')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\teval-logloss:0.66222\ttrain-logloss:0.65630\n",
      "[1]\teval-logloss:0.63606\ttrain-logloss:0.62371\n",
      "[2]\teval-logloss:0.61934\ttrain-logloss:0.59552\n",
      "[3]\teval-logloss:0.60349\ttrain-logloss:0.57184\n",
      "[4]\teval-logloss:0.58581\ttrain-logloss:0.55054\n",
      "[5]\teval-logloss:0.57287\ttrain-logloss:0.53312\n",
      "[6]\teval-logloss:0.55427\ttrain-logloss:0.51397\n",
      "[7]\teval-logloss:0.54133\ttrain-logloss:0.49474\n",
      "[8]\teval-logloss:0.53063\ttrain-logloss:0.47797\n",
      "[9]\teval-logloss:0.51758\ttrain-logloss:0.46147\n",
      "[10]\teval-logloss:0.50393\ttrain-logloss:0.44618\n",
      "[11]\teval-logloss:0.49440\ttrain-logloss:0.43240\n",
      "[12]\teval-logloss:0.48383\ttrain-logloss:0.41847\n",
      "[13]\teval-logloss:0.47076\ttrain-logloss:0.40622\n",
      "[14]\teval-logloss:0.46061\ttrain-logloss:0.39685\n",
      "[15]\teval-logloss:0.45235\ttrain-logloss:0.38514\n",
      "[16]\teval-logloss:0.44622\ttrain-logloss:0.37545\n",
      "[17]\teval-logloss:0.43829\ttrain-logloss:0.36624\n",
      "[18]\teval-logloss:0.43053\ttrain-logloss:0.35843\n",
      "[19]\teval-logloss:0.42350\ttrain-logloss:0.35055\n",
      "[20]\teval-logloss:0.41865\ttrain-logloss:0.34475\n",
      "[21]\teval-logloss:0.41364\ttrain-logloss:0.33671\n",
      "[22]\teval-logloss:0.41022\ttrain-logloss:0.33149\n",
      "[23]\teval-logloss:0.40640\ttrain-logloss:0.32459\n",
      "[24]\teval-logloss:0.40118\ttrain-logloss:0.31836\n",
      "[25]\teval-logloss:0.39496\ttrain-logloss:0.31255\n",
      "[26]\teval-logloss:0.39065\ttrain-logloss:0.30612\n",
      "[27]\teval-logloss:0.38615\ttrain-logloss:0.29971\n",
      "[28]\teval-logloss:0.38236\ttrain-logloss:0.29454\n",
      "[29]\teval-logloss:0.37789\ttrain-logloss:0.29072\n",
      "[30]\teval-logloss:0.37293\ttrain-logloss:0.28631\n",
      "[31]\teval-logloss:0.36926\ttrain-logloss:0.28144\n",
      "[32]\teval-logloss:0.36695\ttrain-logloss:0.27747\n",
      "[33]\teval-logloss:0.36335\ttrain-logloss:0.27331\n",
      "[34]\teval-logloss:0.35827\ttrain-logloss:0.26734\n",
      "[35]\teval-logloss:0.35536\ttrain-logloss:0.26379\n",
      "[36]\teval-logloss:0.35160\ttrain-logloss:0.25880\n",
      "[37]\teval-logloss:0.34915\ttrain-logloss:0.25560\n",
      "[38]\teval-logloss:0.34793\ttrain-logloss:0.25297\n",
      "[39]\teval-logloss:0.34582\ttrain-logloss:0.24890\n",
      "[40]\teval-logloss:0.34155\ttrain-logloss:0.24412\n",
      "[41]\teval-logloss:0.33913\ttrain-logloss:0.24120\n",
      "[42]\teval-logloss:0.33265\ttrain-logloss:0.23650\n",
      "[43]\teval-logloss:0.33055\ttrain-logloss:0.23319\n",
      "[44]\teval-logloss:0.32618\ttrain-logloss:0.22873\n",
      "[45]\teval-logloss:0.32202\ttrain-logloss:0.22460\n",
      "[46]\teval-logloss:0.32014\ttrain-logloss:0.22195\n",
      "[47]\teval-logloss:0.31562\ttrain-logloss:0.21778\n",
      "[48]\teval-logloss:0.31446\ttrain-logloss:0.21540\n",
      "[49]\teval-logloss:0.31441\ttrain-logloss:0.21314\n",
      "[50]\teval-logloss:0.31208\ttrain-logloss:0.20969\n",
      "[51]\teval-logloss:0.31141\ttrain-logloss:0.20746\n",
      "[52]\teval-logloss:0.30982\ttrain-logloss:0.20523\n",
      "[53]\teval-logloss:0.30821\ttrain-logloss:0.20279\n",
      "[54]\teval-logloss:0.30626\ttrain-logloss:0.20060\n",
      "[55]\teval-logloss:0.30547\ttrain-logloss:0.19800\n",
      "[56]\teval-logloss:0.30318\ttrain-logloss:0.19439\n",
      "[57]\teval-logloss:0.29815\ttrain-logloss:0.19029\n",
      "[58]\teval-logloss:0.29707\ttrain-logloss:0.18805\n",
      "[59]\teval-logloss:0.29569\ttrain-logloss:0.18658\n",
      "[60]\teval-logloss:0.29536\ttrain-logloss:0.18504\n",
      "[61]\teval-logloss:0.29405\ttrain-logloss:0.18335\n",
      "[62]\teval-logloss:0.29338\ttrain-logloss:0.18168\n",
      "[63]\teval-logloss:0.29030\ttrain-logloss:0.17960\n",
      "[64]\teval-logloss:0.29003\ttrain-logloss:0.17806\n",
      "[65]\teval-logloss:0.28975\ttrain-logloss:0.17646\n",
      "[66]\teval-logloss:0.28917\ttrain-logloss:0.17515\n",
      "[67]\teval-logloss:0.28691\ttrain-logloss:0.17360\n",
      "[68]\teval-logloss:0.28507\ttrain-logloss:0.17178\n",
      "[69]\teval-logloss:0.28346\ttrain-logloss:0.17007\n",
      "[70]\teval-logloss:0.28087\ttrain-logloss:0.16784\n",
      "[71]\teval-logloss:0.28053\ttrain-logloss:0.16576\n",
      "[72]\teval-logloss:0.28007\ttrain-logloss:0.16411\n",
      "[73]\teval-logloss:0.27778\ttrain-logloss:0.16260\n",
      "[74]\teval-logloss:0.27608\ttrain-logloss:0.16008\n",
      "[75]\teval-logloss:0.27453\ttrain-logloss:0.15806\n",
      "[76]\teval-logloss:0.27344\ttrain-logloss:0.15728\n",
      "[77]\teval-logloss:0.27288\ttrain-logloss:0.15566\n",
      "[78]\teval-logloss:0.27166\ttrain-logloss:0.15476\n",
      "[79]\teval-logloss:0.27005\ttrain-logloss:0.15314\n",
      "[80]\teval-logloss:0.27027\ttrain-logloss:0.15194\n",
      "[81]\teval-logloss:0.26875\ttrain-logloss:0.14983\n",
      "[82]\teval-logloss:0.26909\ttrain-logloss:0.14849\n",
      "[83]\teval-logloss:0.26867\ttrain-logloss:0.14720\n",
      "[84]\teval-logloss:0.26741\ttrain-logloss:0.14559\n",
      "[85]\teval-logloss:0.26502\ttrain-logloss:0.14305\n",
      "[86]\teval-logloss:0.26454\ttrain-logloss:0.14179\n",
      "[87]\teval-logloss:0.26501\ttrain-logloss:0.14009\n",
      "[88]\teval-logloss:0.26474\ttrain-logloss:0.13917\n",
      "[89]\teval-logloss:0.26444\ttrain-logloss:0.13831\n",
      "[90]\teval-logloss:0.26373\ttrain-logloss:0.13722\n",
      "[91]\teval-logloss:0.26304\ttrain-logloss:0.13649\n",
      "[92]\teval-logloss:0.26259\ttrain-logloss:0.13537\n",
      "[93]\teval-logloss:0.26138\ttrain-logloss:0.13310\n",
      "[94]\teval-logloss:0.26078\ttrain-logloss:0.13177\n",
      "[95]\teval-logloss:0.25946\ttrain-logloss:0.12938\n",
      "[96]\teval-logloss:0.25816\ttrain-logloss:0.12840\n",
      "[97]\teval-logloss:0.25772\ttrain-logloss:0.12739\n",
      "[98]\teval-logloss:0.25702\ttrain-logloss:0.12647\n",
      "[99]\teval-logloss:0.25610\ttrain-logloss:0.12502\n",
      "[100]\teval-logloss:0.25546\ttrain-logloss:0.12443\n",
      "[101]\teval-logloss:0.25387\ttrain-logloss:0.12265\n",
      "[102]\teval-logloss:0.25333\ttrain-logloss:0.12204\n",
      "[103]\teval-logloss:0.25237\ttrain-logloss:0.12104\n",
      "[104]\teval-logloss:0.25071\ttrain-logloss:0.11945\n",
      "[105]\teval-logloss:0.25022\ttrain-logloss:0.11777\n",
      "[106]\teval-logloss:0.24963\ttrain-logloss:0.11649\n",
      "[107]\teval-logloss:0.24946\ttrain-logloss:0.11575\n",
      "[108]\teval-logloss:0.24886\ttrain-logloss:0.11419\n",
      "[109]\teval-logloss:0.24892\ttrain-logloss:0.11356\n",
      "[110]\teval-logloss:0.24610\ttrain-logloss:0.11117\n",
      "[111]\teval-logloss:0.24566\ttrain-logloss:0.11029\n",
      "[112]\teval-logloss:0.24478\ttrain-logloss:0.10948\n",
      "[113]\teval-logloss:0.24311\ttrain-logloss:0.10768\n",
      "[114]\teval-logloss:0.24286\ttrain-logloss:0.10691\n",
      "[115]\teval-logloss:0.24232\ttrain-logloss:0.10644\n",
      "[116]\teval-logloss:0.24124\ttrain-logloss:0.10474\n",
      "[117]\teval-logloss:0.24046\ttrain-logloss:0.10392\n",
      "[118]\teval-logloss:0.23837\ttrain-logloss:0.10232\n",
      "[119]\teval-logloss:0.23799\ttrain-logloss:0.10075\n",
      "[120]\teval-logloss:0.23687\ttrain-logloss:0.09980\n",
      "[121]\teval-logloss:0.23713\ttrain-logloss:0.09894\n",
      "[122]\teval-logloss:0.23668\ttrain-logloss:0.09759\n",
      "[123]\teval-logloss:0.23665\ttrain-logloss:0.09697\n",
      "[124]\teval-logloss:0.23605\ttrain-logloss:0.09599\n",
      "[125]\teval-logloss:0.23593\ttrain-logloss:0.09523\n",
      "[126]\teval-logloss:0.23593\ttrain-logloss:0.09468\n",
      "[127]\teval-logloss:0.23518\ttrain-logloss:0.09333\n",
      "[128]\teval-logloss:0.23520\ttrain-logloss:0.09234\n",
      "[129]\teval-logloss:0.23547\ttrain-logloss:0.09171\n",
      "[130]\teval-logloss:0.23562\ttrain-logloss:0.09102\n",
      "[131]\teval-logloss:0.23552\ttrain-logloss:0.09024\n",
      "[132]\teval-logloss:0.23375\ttrain-logloss:0.08907\n",
      "[133]\teval-logloss:0.23328\ttrain-logloss:0.08869\n",
      "[134]\teval-logloss:0.23292\ttrain-logloss:0.08804\n",
      "[135]\teval-logloss:0.23253\ttrain-logloss:0.08764\n",
      "[136]\teval-logloss:0.23252\ttrain-logloss:0.08719\n",
      "[137]\teval-logloss:0.23224\ttrain-logloss:0.08635\n",
      "[138]\teval-logloss:0.23221\ttrain-logloss:0.08572\n",
      "[139]\teval-logloss:0.23103\ttrain-logloss:0.08506\n",
      "[140]\teval-logloss:0.23104\ttrain-logloss:0.08457\n",
      "[141]\teval-logloss:0.23128\ttrain-logloss:0.08391\n",
      "[142]\teval-logloss:0.23139\ttrain-logloss:0.08344\n",
      "[143]\teval-logloss:0.23029\ttrain-logloss:0.08235\n",
      "[144]\teval-logloss:0.23056\ttrain-logloss:0.08185\n",
      "[145]\teval-logloss:0.22995\ttrain-logloss:0.08140\n",
      "[146]\teval-logloss:0.23047\ttrain-logloss:0.08056\n",
      "[147]\teval-logloss:0.22998\ttrain-logloss:0.07958\n",
      "[148]\teval-logloss:0.22920\ttrain-logloss:0.07869\n",
      "[149]\teval-logloss:0.22886\ttrain-logloss:0.07763\n",
      "[150]\teval-logloss:0.22787\ttrain-logloss:0.07681\n",
      "[151]\teval-logloss:0.22847\ttrain-logloss:0.07634\n",
      "[152]\teval-logloss:0.22746\ttrain-logloss:0.07502\n",
      "[153]\teval-logloss:0.22660\ttrain-logloss:0.07442\n",
      "[154]\teval-logloss:0.22593\ttrain-logloss:0.07365\n",
      "[155]\teval-logloss:0.22459\ttrain-logloss:0.07296\n",
      "[156]\teval-logloss:0.22416\ttrain-logloss:0.07242\n",
      "[157]\teval-logloss:0.22408\ttrain-logloss:0.07212\n",
      "[158]\teval-logloss:0.22406\ttrain-logloss:0.07126\n",
      "[159]\teval-logloss:0.22305\ttrain-logloss:0.07004\n",
      "[160]\teval-logloss:0.22268\ttrain-logloss:0.06967\n",
      "[161]\teval-logloss:0.22188\ttrain-logloss:0.06928\n",
      "[162]\teval-logloss:0.22242\ttrain-logloss:0.06881\n",
      "[163]\teval-logloss:0.22307\ttrain-logloss:0.06808\n",
      "[164]\teval-logloss:0.22161\ttrain-logloss:0.06719\n",
      "[165]\teval-logloss:0.22124\ttrain-logloss:0.06648\n",
      "[166]\teval-logloss:0.22103\ttrain-logloss:0.06594\n",
      "[167]\teval-logloss:0.22097\ttrain-logloss:0.06555\n",
      "[168]\teval-logloss:0.22109\ttrain-logloss:0.06493\n",
      "[169]\teval-logloss:0.22143\ttrain-logloss:0.06462\n",
      "[170]\teval-logloss:0.22148\ttrain-logloss:0.06421\n",
      "[171]\teval-logloss:0.22181\ttrain-logloss:0.06380\n",
      "[172]\teval-logloss:0.22202\ttrain-logloss:0.06361\n",
      "[173]\teval-logloss:0.22151\ttrain-logloss:0.06297\n",
      "[174]\teval-logloss:0.22127\ttrain-logloss:0.06243\n",
      "[175]\teval-logloss:0.22188\ttrain-logloss:0.06213\n",
      "[176]\teval-logloss:0.22024\ttrain-logloss:0.06102\n",
      "[177]\teval-logloss:0.21848\ttrain-logloss:0.06035\n",
      "[178]\teval-logloss:0.21786\ttrain-logloss:0.05951\n",
      "[179]\teval-logloss:0.21780\ttrain-logloss:0.05866\n",
      "[180]\teval-logloss:0.21834\ttrain-logloss:0.05830\n",
      "[181]\teval-logloss:0.21735\ttrain-logloss:0.05762\n",
      "[182]\teval-logloss:0.21631\ttrain-logloss:0.05723\n",
      "[183]\teval-logloss:0.21685\ttrain-logloss:0.05677\n",
      "[184]\teval-logloss:0.21661\ttrain-logloss:0.05605\n",
      "[185]\teval-logloss:0.21683\ttrain-logloss:0.05573\n",
      "[186]\teval-logloss:0.21658\ttrain-logloss:0.05546\n",
      "[187]\teval-logloss:0.21610\ttrain-logloss:0.05488\n",
      "[188]\teval-logloss:0.21581\ttrain-logloss:0.05454\n",
      "[189]\teval-logloss:0.21478\ttrain-logloss:0.05411\n",
      "[190]\teval-logloss:0.21348\ttrain-logloss:0.05336\n",
      "[191]\teval-logloss:0.21280\ttrain-logloss:0.05299\n",
      "[192]\teval-logloss:0.21220\ttrain-logloss:0.05247\n",
      "[193]\teval-logloss:0.21222\ttrain-logloss:0.05226\n",
      "[194]\teval-logloss:0.21240\ttrain-logloss:0.05156\n",
      "[195]\teval-logloss:0.21230\ttrain-logloss:0.05112\n",
      "[196]\teval-logloss:0.21267\ttrain-logloss:0.05080\n",
      "[197]\teval-logloss:0.21183\ttrain-logloss:0.05054\n",
      "[198]\teval-logloss:0.21217\ttrain-logloss:0.05027\n",
      "[199]\teval-logloss:0.21154\ttrain-logloss:0.04947\n"
     ]
    }
   ],
   "source": [
    "# Extract num_boost_round from params for clarity\n",
    "num_boost_round = params.pop('num_boost_round')\n",
    "\n",
    "# Train the model with early stopping\n",
    "bst = xgb.train(\n",
    "    params,\n",
    "    d_train,\n",
    "    num_boost_round = num_boost_round,  # Set a high number to allow early stopping\n",
    "    evals=evals,\n",
    "    early_stopping_rounds=10,  # Stop training if no improvement after 10 rounds\n",
    "    verbose_eval=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions\n",
    "\n",
    "y_pred = bst.predict(d_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Score: 0.9033333333333333\n",
      "\n",
      "Classification report:  \n",
      "\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.92      0.90      0.91       160\n",
      "         1.0       0.89      0.91      0.90       140\n",
      "\n",
      "    accuracy                           0.90       300\n",
      "   macro avg       0.90      0.90      0.90       300\n",
      "weighted avg       0.90      0.90      0.90       300\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model\n",
    "\n",
    "# Extract the true labels from DMatrix\n",
    "y_true = d_test.get_label()\n",
    "\n",
    "\n",
    "# Convert predicted probabilities to binary values (e.g., 1 if probability > 0.5)\n",
    "y_pred_binary = (y_pred > 0.5).astype(int)\n",
    "\n",
    "\n",
    "accuracy = accuracy_score(y_true,y_pred_binary)\n",
    "\n",
    "report = classification_report(y_true,y_pred_binary)\n",
    "\n",
    "print ( f\"Accuracy Score: {accuracy}\\n\")\n",
    "\n",
    "print ( f\"Classification report:  \\n\")\n",
    "\n",
    "print (f\" {report}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "masterxml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
