{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import lightgbm as lgb\n",
    "\n",
    "from sklearn.datasets import make_classification\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.metrics import log_loss\n",
    "\n",
    "# from bayes_opt import BayesianOptimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate synthetic dataset\n",
    "\n",
    "X,y = make_classification(n_samples=5000, n_features=20, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y, test_size=0.2,random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to LightGBM dataset\n",
    "\n",
    "\n",
    "train_data = lgb.Dataset(X_train, label=y_train)\n",
    "\n",
    "test_data = lgb.Dataset(X_test, label=y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Custom Loss Functions in LightGBM\n",
    "\n",
    "By default, LightGBM uses predefined loss functions like binary log loss or MSE.\n",
    "But we can define our own custom loss function to guide optimization.\n",
    "\n",
    "\n",
    "define a custom log loss function and its gradient & hessian for LightGBM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom Log Loss function (for binary classification)\n",
    "\n",
    "def custom_log_loss(y_true,y_pred):\n",
    "\n",
    "    eps = 1e-15\n",
    "\n",
    "    y_pred = np.clip(y_pred, eps, 1-eps)\n",
    "\n",
    "    # Gradient (first derivative)\n",
    "\n",
    "    grad = y_pred - y_true\n",
    "\n",
    "    # Hessian (second derivative)\n",
    "\n",
    "    hess = y_pred * (1 - y_pred)\n",
    "\n",
    "    return grad,hess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train model with custom loss\n",
    "params = {\n",
    "\n",
    "\n",
    "    'objective': 'binary',\n",
    "    'metric': 'binary_logloss',\n",
    "    'learning_rate': 0.05,\n",
    "    'num_leaves': 31\n",
    "\n",
    "    \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train.astype(np.float32)\n",
    "\n",
    "X_test = X_test.astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 2008, number of negative: 1992\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000592 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5100\n",
      "[LightGBM] [Info] Number of data points in the train set: 4000, number of used features: 20\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.502000 -> initscore=0.008000\n",
      "[LightGBM] [Info] Start training from score 0.008000\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "[50]\tvalid_0's binary_logloss: 0.217242\n",
      "[100]\tvalid_0's binary_logloss: 0.200936\n",
      "Early stopping, best iteration is:\n",
      "[76]\tvalid_0's binary_logloss: 0.20056\n"
     ]
    }
   ],
   "source": [
    "model = lgb.train(\n",
    "    params, \n",
    "    train_data, \n",
    "    valid_sets=[test_data], \n",
    "    num_boost_round=200,\n",
    "    # fobj=custom_log_loss,\n",
    "\n",
    "    callbacks=[\n",
    "        lgb.early_stopping(stopping_rounds=30),  \n",
    "        lgb.log_evaluation(period=50)  \n",
    "    ]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predictions\n",
    "\n",
    "y_pred = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Custom Log Loss: 0.2006\n"
     ]
    }
   ],
   "source": [
    "loss = log_loss(y_test, y_pred)\n",
    "\n",
    "print(f\"Custom Log Loss: {loss:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " 2. Bayesian Optimization for Hyperparameter Tuning\n",
    " \n",
    " \n",
    " Instead of grid search or random search, we use Bayesian Optimization for smarter tuning.\n",
    "It learns from past trials and selects better hyperparameters efficiently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define objective function\n",
    "\n",
    "def lgb_evaluate(num_leaves, learning_rate, max_depth):\n",
    "\n",
    "    hyper_params = {\n",
    "        \n",
    "        'objective' : 'binary',\n",
    "        'metric'    : 'binary_error',\n",
    "        'boosting_type':'gbdt',\n",
    "        'num_leaves' : int(num_leaves),\n",
    "        'learning_rate': learning_rate,\n",
    "        'max_depth': int(max_depth),\n",
    "        'verbose': -1\n",
    "    }\n",
    "\n",
    "    hyper_model = lgb.train(\n",
    "    params, \n",
    "    train_data, \n",
    "    valid_sets=[test_data], \n",
    "    num_boost_round=100, \n",
    "    callbacks=[\n",
    "        lgb.early_stopping(stopping_rounds=10),  \n",
    "        lgb.log_evaluation(period=False)  \n",
    "    ]\n",
    "    )\n",
    "\n",
    "    hyper_preds = hyper_model.predict(X_test)\n",
    "\n",
    "    hyper_accuracy = np.mean(( hyper_preds > 0.5) == y_test)\n",
    "\n",
    "    return hyper_accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define search space\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "masterxml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
