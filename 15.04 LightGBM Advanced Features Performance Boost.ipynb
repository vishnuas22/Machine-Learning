{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import lightgbm as lgb\n",
    "\n",
    "import scipy.sparse\n",
    "\n",
    "from sklearn.datasets import make_classification\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.metrics import log_loss\n",
    "\n",
    "from bayes_opt import BayesianOptimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate synthetic dataset\n",
    "\n",
    "X,y = make_classification(n_samples=5000, n_features=20, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y, test_size=0.2,random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to LightGBM dataset\n",
    "\n",
    "\n",
    "train_data = lgb.Dataset(X_train, label=y_train)\n",
    "\n",
    "test_data = lgb.Dataset(X_test, label=y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Custom Loss Functions in LightGBM\n",
    "\n",
    "By default, LightGBM uses predefined loss functions like binary log loss or MSE.\n",
    "But we can define our own custom loss function to guide optimization.\n",
    "\n",
    "\n",
    "define a custom log loss function and its gradient & hessian for LightGBM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom Log Loss function (for binary classification)\n",
    "\n",
    "def custom_log_loss(y_true,y_pred):\n",
    "\n",
    "    eps = 1e-15\n",
    "\n",
    "    y_pred = np.clip(y_pred, eps, 1-eps)\n",
    "\n",
    "    # Gradient (first derivative)\n",
    "\n",
    "    grad = y_pred - y_true\n",
    "\n",
    "    # Hessian (second derivative)\n",
    "\n",
    "    hess = y_pred * (1 - y_pred)\n",
    "\n",
    "    return grad,hess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train model with custom loss\n",
    "params = {\n",
    "\n",
    "\n",
    "    'objective': 'binary',\n",
    "    'metric': 'binary_logloss',\n",
    "    'learning_rate': 0.05,\n",
    "    'num_leaves': 31\n",
    "\n",
    "    \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train.astype(np.float32)\n",
    "\n",
    "X_test = X_test.astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 30 rounds\n",
      "[50]\tvalid_0's binary_logloss: 0.217242\n",
      "[100]\tvalid_0's binary_logloss: 0.200936\n",
      "Early stopping, best iteration is:\n",
      "[76]\tvalid_0's binary_logloss: 0.20056\n"
     ]
    }
   ],
   "source": [
    "model = lgb.train(\n",
    "    params, \n",
    "    train_data, \n",
    "    valid_sets=[test_data], \n",
    "    num_boost_round=200,\n",
    "    callbacks=[\n",
    "        lgb.early_stopping(stopping_rounds=30),  \n",
    "        lgb.log_evaluation(period=50)  \n",
    "    ]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predictions\n",
    "\n",
    "y_pred = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Custom Log Loss: 0.2006\n"
     ]
    }
   ],
   "source": [
    "loss = log_loss(y_test, y_pred)\n",
    "\n",
    "print(f\"Custom Log Loss: {loss:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " 2. Bayesian Optimization for Hyperparameter Tuning\n",
    " \n",
    " \n",
    " Instead of grid search or random search, we use Bayesian Optimization for smarter tuning.\n",
    "It learns from past trials and selects better hyperparameters efficiently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define objective function\n",
    "\n",
    "def lgb_evaluate(num_leaves, learning_rate, max_depth):\n",
    "\n",
    "    hyper_params = {\n",
    "        \n",
    "        'objective' : 'binary',\n",
    "        'metric'    : 'binary_error',\n",
    "        'boosting_type':'gbdt',\n",
    "        'num_leaves' : int(num_leaves),\n",
    "        'learning_rate': learning_rate,\n",
    "        'max_depth': int(max_depth),\n",
    "        'verbose': -1\n",
    "    }\n",
    "\n",
    "    hyper_model = lgb.train(\n",
    "    hyper_params, \n",
    "    train_data, \n",
    "    valid_sets=[test_data], \n",
    "    num_boost_round=100, \n",
    "    callbacks=[\n",
    "        lgb.early_stopping(stopping_rounds=10),  \n",
    "        lgb.log_evaluation(period=False)  \n",
    "    ]\n",
    "    )\n",
    "\n",
    "    hyper_preds = hyper_model.predict(X_test)\n",
    "\n",
    "    hyper_accuracy = np.mean(( hyper_preds > 0.5) == y_test)\n",
    "\n",
    "    return hyper_accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define search space\n",
    "\n",
    "optimizer = BayesianOptimization(\n",
    "\n",
    "      f=lgb_evaluate,\n",
    "      pbounds={\n",
    "         'num_leaves': (10,100),\n",
    "         'learning_rate':(0.01,0.3),\n",
    "         'max_depth': (3,12)\n",
    "\n",
    "       },\n",
    "    random_state=42\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|   iter    |  target   | learni... | max_depth | num_le... |\n",
      "-------------------------------------------------------------\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "Early stopping, best iteration is:\n",
      "[11]\tvalid_0's binary_error: 0.07\n",
      "| \u001b[39m1        \u001b[39m | \u001b[39m0.93     \u001b[39m | \u001b[39m0.1186   \u001b[39m | \u001b[39m11.56    \u001b[39m | \u001b[39m75.88    \u001b[39m |\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "Early stopping, best iteration is:\n",
      "[3]\tvalid_0's binary_error: 0.079\n",
      "| \u001b[39m2        \u001b[39m | \u001b[39m0.921    \u001b[39m | \u001b[39m0.1836   \u001b[39m | \u001b[39m4.404    \u001b[39m | \u001b[39m24.04    \u001b[39m |\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "Early stopping, best iteration is:\n",
      "[14]\tvalid_0's binary_error: 0.07\n",
      "| \u001b[39m3        \u001b[39m | \u001b[39m0.93     \u001b[39m | \u001b[39m0.02684  \u001b[39m | \u001b[39m10.8     \u001b[39m | \u001b[39m64.1     \u001b[39m |\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "Early stopping, best iteration is:\n",
      "[14]\tvalid_0's binary_error: 0.083\n",
      "| \u001b[39m4        \u001b[39m | \u001b[39m0.917    \u001b[39m | \u001b[39m0.2153   \u001b[39m | \u001b[39m3.185    \u001b[39m | \u001b[39m97.29    \u001b[39m |\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "Early stopping, best iteration is:\n",
      "[18]\tvalid_0's binary_error: 0.077\n",
      "| \u001b[39m5        \u001b[39m | \u001b[39m0.923    \u001b[39m | \u001b[39m0.2514   \u001b[39m | \u001b[39m4.911    \u001b[39m | \u001b[39m26.36    \u001b[39m |\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "Early stopping, best iteration is:\n",
      "[29]\tvalid_0's binary_error: 0.083\n",
      "| \u001b[39m6        \u001b[39m | \u001b[39m0.917    \u001b[39m | \u001b[39m0.1526   \u001b[39m | \u001b[39m3.176    \u001b[39m | \u001b[39m70.42    \u001b[39m |\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "Early stopping, best iteration is:\n",
      "[6]\tvalid_0's binary_error: 0.072\n",
      "| \u001b[39m7        \u001b[39m | \u001b[39m0.928    \u001b[39m | \u001b[39m0.2815   \u001b[39m | \u001b[39m11.98    \u001b[39m | \u001b[39m69.71    \u001b[39m |\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "Early stopping, best iteration is:\n",
      "[5]\tvalid_0's binary_error: 0.07\n",
      "| \u001b[39m8        \u001b[39m | \u001b[39m0.93     \u001b[39m | \u001b[39m0.1112   \u001b[39m | \u001b[39m11.92    \u001b[39m | \u001b[39m58.46    \u001b[39m |\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "Early stopping, best iteration is:\n",
      "[4]\tvalid_0's binary_error: 0.074\n",
      "| \u001b[39m9        \u001b[39m | \u001b[39m0.926    \u001b[39m | \u001b[39m0.08287  \u001b[39m | \u001b[39m11.84    \u001b[39m | \u001b[39m83.31    \u001b[39m |\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "Early stopping, best iteration is:\n",
      "[12]\tvalid_0's binary_error: 0.07\n",
      "| \u001b[39m10       \u001b[39m | \u001b[39m0.93     \u001b[39m | \u001b[39m0.06036  \u001b[39m | \u001b[39m5.024    \u001b[39m | \u001b[39m53.34    \u001b[39m |\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "Early stopping, best iteration is:\n",
      "[12]\tvalid_0's binary_error: 0.073\n",
      "| \u001b[39m11       \u001b[39m | \u001b[39m0.927    \u001b[39m | \u001b[39m0.1359   \u001b[39m | \u001b[39m11.57    \u001b[39m | \u001b[39m47.62    \u001b[39m |\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "Early stopping, best iteration is:\n",
      "[33]\tvalid_0's binary_error: 0.073\n",
      "| \u001b[39m12       \u001b[39m | \u001b[39m0.927    \u001b[39m | \u001b[39m0.2341   \u001b[39m | \u001b[39m3.045    \u001b[39m | \u001b[39m43.75    \u001b[39m |\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "Early stopping, best iteration is:\n",
      "[12]\tvalid_0's binary_error: 0.07\n",
      "| \u001b[39m13       \u001b[39m | \u001b[39m0.93     \u001b[39m | \u001b[39m0.1828   \u001b[39m | \u001b[39m11.98    \u001b[39m | \u001b[39m35.73    \u001b[39m |\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "Early stopping, best iteration is:\n",
      "[26]\tvalid_0's binary_error: 0.072\n",
      "| \u001b[39m14       \u001b[39m | \u001b[39m0.928    \u001b[39m | \u001b[39m0.1248   \u001b[39m | \u001b[39m11.83    \u001b[39m | \u001b[39m10.03    \u001b[39m |\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "Early stopping, best iteration is:\n",
      "[4]\tvalid_0's binary_error: 0.073\n",
      "| \u001b[39m15       \u001b[39m | \u001b[39m0.927    \u001b[39m | \u001b[39m0.1726   \u001b[39m | \u001b[39m5.645    \u001b[39m | \u001b[39m59.12    \u001b[39m |\n",
      "=============================================================\n"
     ]
    }
   ],
   "source": [
    "# Run optimization\n",
    "\n",
    "optimizer.maximize(init_points=5, n_iter=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters : {'target': 0.93, 'params': {'learning_rate': 0.11861663446573512, 'max_depth': 11.556428757689245, 'num_leaves': 75.87945476302646}}\n"
     ]
    }
   ],
   "source": [
    "# Print best parameters\n",
    "\n",
    "print(\"Best Parameters :\", optimizer.max)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LightGBM allows monotonic constraints, ensuring that a feature’s impact is always increasing or decreasing.\n",
    "Example: Ensuring Price Always Increases with Age"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "✅ Why Monotonic Constraints?\n",
    "\n",
    "Prevent counterintuitive results (e.g., higher salary should not decrease approval rate)\n",
    "Adds trust to the model for business applications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mono_params = {\n",
    "\n",
    "    'objective': 'regression',\n",
    "    'metrics' : 'rmse',\n",
    "    'monotone_constraints' : [1, 0, -1]  # 1: Increasing, 0: No constraint, -1: Decreasing\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mono_model = lgb.train(\n",
    "\n",
    "\n",
    "    mono_params,\n",
    "    train_data,\n",
    "    valid_sets=[test_data],\n",
    "    num_boost_round=100,\n",
    "    callbacks=[\n",
    "        lgb.log_evaluation(period=50)\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LightGBM efficiently handles missing values & sparse data without requiring imputation.\n",
    "Example: Creating Sparse Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert dataset to sparse matrix\n",
    "\n",
    "X_train_sparse = scipy.sparse.csr_matrix(X_train)\n",
    "\n",
    "X_test_sparse = scipy.sparse.csr_matrix(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train LightGBM on sparse data\n",
    "\n",
    "train_data_sparse = lgb.Dataset(X_train_sparse, label=y_train)\n",
    "\n",
    "test_data_sparse = lgb.Dataset(X_test_sparse, label=y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train model\n",
    "\n",
    "model = lgb.train(\n",
    "    params, \n",
    "    train_data_sparse, \n",
    "    valid_sets=[test_data_sparse], \n",
    "    num_boost_round=100\n",
    "    \n",
    "    )\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "masterxml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
