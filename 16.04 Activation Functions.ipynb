{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Neurons in a neural network take the weighted sum of inputs (which we computed as‚àí1.9) and pass it through an activation function to introduce non-linearity.\n",
    "\n",
    "Without activation functions, deep networks would collapse into simple linear models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1Ô∏è‚É£ Step Function (Threshold Activation)\n",
    "\n",
    "üëâ A step function outputs either 0 or 1 based on a threshold.\n",
    "\n",
    "‚úÖ Used in: Early perceptrons (not used in modern deep learning).\n",
    "‚úÖ Example Calculation for \n",
    "\n",
    "x=‚àí1.9:\n",
    "Since ‚àí1.9<0, output = 0."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2Ô∏è‚É£ Sigmoid Activation (Logistic Function)\n",
    "\n",
    "üëâ Squashes values into the range (0,1), useful for probabilities.\n",
    "\n",
    "‚úÖ Used in: Binary classification problems.\n",
    "‚úÖ Example Calculation for \n",
    "x=‚àí1.9:\n",
    "\n",
    "üîπ Issue: Can suffer from vanishing gradients for large positive/negative inputs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3Ô∏è‚É£ Tanh (Hyperbolic Tangent)\n",
    "\n",
    "üëâ Similar to sigmoid but ranges from -1 to 1, making it zero-centered.\n",
    "\n",
    "‚úÖ Used in: RNNs, deep networks.\n",
    "‚úÖ Example Calculation for \n",
    "\n",
    "x=‚àí1.9:\n",
    "\n",
    "üîπ Issue: Still suffers from vanishing gradients."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4Ô∏è‚É£ ReLU (Rectified Linear Unit)\n",
    "üëâ The most commonly used activation function in deep learning.\n",
    "\n",
    "f(x)=max(0,x)\n",
    "\n",
    "‚úÖ Used in: CNNs, MLPs, modern deep networks.\n",
    "\n",
    "‚úÖ Example Calculation for \n",
    "\n",
    "x=‚àí1.9:\n",
    "\n",
    "üîπ Issue: Dead neurons problem (negative values always become zero)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5Ô∏è‚É£ Leaky ReLU\n",
    "\n",
    "üëâ Fixes ReLU‚Äôs dead neuron issue by allowing small negative values.\n",
    "\n",
    "f(x)=max(0.01x,x)\n",
    "\n",
    "‚úÖ Used in: Variants of deep networks.\n",
    "‚úÖ Example Calculation for \n",
    "\n",
    "x=‚àí1.9:\n",
    "\n",
    "f(‚àí1.9)=0.01√ó(‚àí1.9)=‚àí0.019"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6Ô∏è‚É£ Softmax (For Multi-Class Classification)\n",
    "\n",
    "üëâ Converts raw scores into probabilities summing to 1.\n",
    "\n",
    "‚úÖ Used in: Last layer of a multi-class classification network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Which Activation Function to Use?\n",
    "\n",
    "üîπ Binary Classification? ‚Üí Sigmoid\n",
    "\n",
    "üîπ Multi-Class Classification? ‚Üí Softmax\n",
    "\n",
    "üîπ Hidden Layers in Deep Networks? ‚Üí ReLU or Leaky ReLU\n",
    "\n",
    "üîπ Recurrent Networks (RNNs)? ‚Üí Tanh or ReLU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def step_function(x):\n",
    "\n",
    "    return 1 if x >=0 else 0\n",
    "\n",
    "def sigmoid(x):\n",
    "\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def tanh(x):\n",
    "\n",
    "    return np.tanh(x)\n",
    "\n",
    "def relu(x):\n",
    "\n",
    "    return max(0,x)\n",
    "\n",
    "def leaky_relu(x, alpha=0.1):\n",
    "\n",
    "    return x if x > 0 else alpha * x\n",
    "\n",
    "\n",
    "# input_value\n",
    "\n",
    "x = -1.9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step function: 0\n",
      "Sigmoid: 0.13010847436299786\n",
      "Tanh: -0.9562374581277391\n",
      "ReLU: 0\n",
      "Leaky ReLU: -0.19\n"
     ]
    }
   ],
   "source": [
    "# Compute activations in Python\n",
    "\n",
    "print(f'Step function: {step_function(x)}')\n",
    "\n",
    "print(f'Sigmoid: {sigmoid(x)}')\n",
    "\n",
    "print(f'Tanh: {tanh(x)}')\n",
    "\n",
    "print(f'ReLU: {relu(x)}')\n",
    "\n",
    "print(f'Leaky ReLU: {leaky_relu(x)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PyTorch Tensor\n",
    "\n",
    "x_torch = torch.tensor(x, dtype=torch.float32 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "PyTorch Implementations\n",
      "Sigmoid : 0.13010847568511963\n",
      "Tanh : -0.9562374353408813\n",
      "ReLU : 0.0\n",
      "Leaky ReLU : -0.01899999938905239\n"
     ]
    }
   ],
   "source": [
    "# Compute activations in PyTorch\n",
    "\n",
    "print(\"\\nPyTorch Implementations\")\n",
    "\n",
    "print(\"Sigmoid :\", torch.sigmoid(x_torch).item())\n",
    "\n",
    "print(\"Tanh :\", torch.tanh(x_torch).item())\n",
    "\n",
    "print(\"ReLU :\", F.relu(x_torch).item())\n",
    "\n",
    "print(\"Leaky ReLU :\", F.leaky_relu(x_torch, negative_slope = 0.01).item())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "masterxdl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
